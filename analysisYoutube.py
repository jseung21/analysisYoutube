# 1. Konlpy Import First to avoid JVM/DLL conflicts
from konlpy.tag import Okt  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ëª…ì‚¬ ì¶”ì¶œìš©)
from collections import Counter  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
from wordcloud import WordCloud  # ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™” ìƒì„±
import torch  # ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (Transformers ëª¨ë¸ êµ¬ë™ìš©)
from transformers import pipeline  # Hugging Faceì˜ NLP íŒŒì´í”„ë¼ì¸ (ê°ì„± ë¶„ì„ ë“±)

import streamlit as st  # ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ ë° ì¡°ì‘
import matplotlib.pyplot as plt  # ë°ì´í„° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib
matplotlib.use('Agg') # Streamlit í™˜ê²½ì—ì„œ GUI ì—ëŸ¬ ë°©ì§€ (ë°±ì—”ë“œ ì„¤ì •)
import seaborn as sns  # Matplotlib ê¸°ë°˜ì˜ í†µê³„ì  ì‹œê°í™”
from googleapiclient.discovery import build  # Google API í´ë¼ì´ì–¸íŠ¸ (YouTube Data API ì—°ë™)
import re  # ì •ê·œí‘œí˜„ì‹ (í…ìŠ¤íŠ¸ ì •ì œìš©)
import os  # ìš´ì˜ì²´ì œ ìƒí˜¸ì‘ìš© (íŒŒì¼ ê²½ë¡œ í™•ì¸ ë“±)

from step_12 import run_step12

# ==========================================
# [ê°œìš”]
# ì´ íŒŒì¼ì€ Streamlitì„ ì‚¬ìš©í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤.
# ìœ íŠœë¸Œ ë¹„ë””ì˜¤ IDë¥¼ ì…ë ¥ë°›ì•„ ë‹¤ìŒ ë‹¨ê³„ë“¤ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
# 1. step_12.pyì˜ ê¸°ëŠ¥ì„ ì´ìš©í•´ ì˜ìƒ ìš”ì•½, í‚¤ì›Œë“œ ì¶”ì¶œ, ì£¼ì œ ë¶„ë¥˜ (LLM í™œìš©)
# 2. Google YouTube Data APIë¥¼ í†µí•´ ì˜ìƒì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘
# 3. ìˆ˜ì§‘ëœ ëŒ“ê¸€ì— ëŒ€í•´ ê°ì„± ë¶„ì„(Sentiment Analysis) ë° í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰
# 4. ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì´ ì°¨íŠ¸, ì›Œë“œ í´ë¼ìš°ë“œ ë“±ìœ¼ë¡œ ì‹œê°í™” ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì œê³µ
# ==========================================


# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•¨)
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ë¶„ì„ê¸°",
    page_icon="ğŸ¬",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
font_path = 'C:/Windows/Fonts/malgun.ttf'
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False

# ë§¥ìš© í•œê¸€ í°íŠ¸
#import matplotlib.font_manager as fm
#font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'
#font_name = fm.FontProperties(fname=font_path).get_name()
#plt.rc('font', family=font_name)
#plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# 1. í•¨ìˆ˜ ì •ì˜ (ìºì‹± ì ìš©)
# ==========================================

@st.cache_resource
def load_sentiment_model():
    """
    [í•¨ìˆ˜] load_sentiment_model
    ë¡œì»¬ ë””ë ‰í† ë¦¬("./my_model")ì— ì €ì¥ëœ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ Hugging Face íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    @st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì„¸ì…˜ ì „ì²´ì—ì„œ ê³µìœ í•©ë‹ˆë‹¤.
    
    Returns:
        pipeline: Hugging Faceì˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜(text-classification) íŒŒì´í”„ë¼ì¸ ê°ì²´
    """
    # ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬("./my_model")ì—ì„œ ë¡œë“œ (download_model.pyë¡œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ í•„ìš”)
    return pipeline("text-classification", model="./my_model")


@st.cache_data
def get_video_comments(api_key, video_id, max_results=100):
    """
    [í•¨ìˆ˜] get_video_comments
    YouTube Data API v3ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    @st.cache_data: ë™ì¼í•œ ì…ë ¥(api_key, video_id ë“±)ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬API í˜¸ì¶œ ë¹„ìš©ì„ ì ˆì•½í•˜ê³  ì‘ë‹µ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    
    Args:
        api_key (str): êµ¬ê¸€ ê°œë°œì ì½˜ì†”ì—ì„œ ë°œê¸‰ë°›ì€ ìœ íŠœë¸Œ API í‚¤
        video_id (str): ëŒ“ê¸€ì„ ìˆ˜ì§‘í•  ìœ íŠœë¸Œ ì˜ìƒ ID
        max_results (int): ìˆ˜ì§‘í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (ê¸°ë³¸ê°’ 100)
    
    Returns:
        pd.DataFrame: ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë°ì´í„°(ë‚ ì§œ, ì‘ì„±ì, ë‚´ìš©)ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„
    """

    try:
        youtube = build('youtube', 'v3', developerKey=api_key)
        comments = []
        
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=min(max_results, 100), # API í•œ ë²ˆ ìš”ì²­ ìµœëŒ€ 100ê°œ
            textFormat="plainText"
        )

        while request and len(comments) < max_results:
            response = request.execute()
            
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
                date = item['snippet']['topLevelComment']['snippet']['publishedAt']
                comments.append([date, author, comment])
                
            if 'nextPageToken' in response and len(comments) < max_results:
                request = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=min(max_results - len(comments), 100),
                    textFormat="plainText",
                    pageToken=response['nextPageToken']
                )
            else:
                break
        
        pd.DataFrame(comments, columns=['Date', 'Author', 'Comment']).to_csv('test.csv')
        return pd.DataFrame(comments, columns=['Date', 'Author', 'Comment'])
    except Exception as e:
        st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

@st.cache_data
def analyze_comments(df):
    """
    [í•¨ìˆ˜] analyze_comments
    ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, í˜•íƒœì†Œ ë¶„ì„, ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    @st.cache_data: ë™ì¼í•œ ì…ë ¥(df)ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬ ì‘ë‹µ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): 'Comment' ì»¬ëŸ¼ì´ í¬í•¨ëœ ëŒ“ê¸€ ë°ì´í„°í”„ë ˆì„
        
    Returns:
        result_df (pd.DataFrame): ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        all_nouns (list): ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±ì„ ìœ„í•œ ì¶”ì¶œëœ ëª¨ë“  ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (Konlpyì˜ Okt ì‚¬ìš©)
    okt = Okt()
    
    valid_comments = []
    valid_dates = []
    valid_authors = []
    sentiments = []
    all_nouns = []

    # ì§„í–‰ë¥  í‘œì‹œì¤„
    progress_bar = st.progress(0)
    total_rows = len(df)

    for i, row in df.iterrows():
        comment = row['Comment']
        # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì´ëª¨ì§€ ë“±)
        clean_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', comment)
        
        if not clean_text.strip():
            continue
            
        # 1. ëª…ì‚¬ ì¶”ì¶œ (ì›Œë“œí´ë¼ìš°ë“œ ìš©)
        nouns = okt.nouns(clean_text)
        all_nouns.extend([n for n in nouns if len(n) > 1])

        # 2. AI ê°ì„± ë¶„ì„
        # ëª¨ë¸ì€ ì…ë ¥ ê¸¸ì´ ì œí•œ(ë³´í†µ 512 í† í°)ì´ ìˆìœ¼ë¯€ë¡œ, ì•ˆì „í•˜ê²Œ ì•ë¶€ë¶„ 512ìë§Œ ì˜ë¼ì„œ ë¶„ì„
        try:
            result = sentiment_classifier(clean_text[:512])[0] 
            label = result['label']
            score = result['score'] # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ê°’ (í˜„ì¬ ë¡œì§ì—ì„œëŠ” ë¼ë²¨ ê²°ì •ì—ë§Œ ì‚¬ìš©ë¨)

            if label == 'LABEL_1':
                sentiment = 'Positive'
            else:
                sentiment = 'Negative'
        except Exception:
            sentiment = 'Neutral' # ì˜¤ë¥˜ ì‹œ ì¤‘ë¦½ 
        
        valid_comments.append(clean_text)
        valid_dates.append(row['Date'])
        valid_authors.append(row['Author'])
        sentiments.append(sentiment)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if (i + 1) % 10 == 0 or (i + 1) == total_rows:
            progress_bar.progress((i + 1) / total_rows)

    progress_bar.empty() # ì™„ë£Œ í›„ ì œê±°

    result_df = pd.DataFrame({
        'Date': valid_dates,
        'Author': valid_authors,
        'Comment': valid_comments,
        'Sentiment': sentiments
    })
    
    return result_df, all_nouns


# ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”
sentiment_classifier = load_sentiment_model()

# ==========================================
# 2. UI êµ¬ì„±
# ==========================================

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    api_key_input = st.text_input("YouTube API Key", value='AIzaSyDD4Kw6X4RlToeRp1YkwJG0LRW6izBr9JU', type="password")
    max_comments = st.slider("ìˆ˜ì§‘í•  ëŒ“ê¸€ ìˆ˜", min_value=10, max_value=1000, value=200, step=10)
    st.info("API KeyëŠ” ê¸°ë³¸ê°’ì´ ì…ë ¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í™”ë©´
st.title("ğŸ¬ AI ê¸°ë°˜ YouTube ì½˜í…ì¸  ìë™ ë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("""
### ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

**STEP 1** : ìœ íŠœë¸Œ ì˜ìƒì˜ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.  

**STEP 2** : ìš”ì•½ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìœ íŠœë¸Œ ì£¼ì œë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.  

**STEP 3** : ìœ íŠœë¸Œ ì˜ìƒì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬  
- **ê¸ì • / ë¶€ì • ì—¬ë¡ **ì„ ë¶„ì„í•˜ê³   
- ì£¼ìš” í‚¤ì›Œë“œë¥¼ **ì›Œë“œí´ë¼ìš°ë“œ**ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
""")

video_id_input = st.text_input("YouTube Video ID ë˜ëŠ” URL ì…ë ¥", value="QkGkE9jRX_g")

# URLì—ì„œ ID ì¶”ì¶œ ë¡œì§
# URL íŒŒì‹± ë¡œì§: ì‚¬ìš©ìê°€ ì „ì²´ URLì„ ì…ë ¥í–ˆë“ , ë‹¨ì¶• URL(youtu.be)ì„ ì…ë ¥í–ˆë“  IDë§Œ ì¶”ì¶œ
if "youtube.com" in video_id_input or "youtu.be" in video_id_input:
    if "v=" in video_id_input:
        video_id = video_id_input.split("v=")[1].split("&")[0]
    elif "youtu.be" in video_id_input:
        video_id = video_id_input.split("/")[-1]
    else:
        video_id = video_id_input
else:
    video_id = video_id_input

# ë¶„ì„(step_12)ì„ ìœ„í•´ ì „ì²´ URL ì¬êµ¬ì„±
video_url = f'https://www.youtube.com/watch?v={video_id}'

if st.button("ë¶„ì„ ì‹œì‘ ğŸš€", type="primary"):
    if not api_key_input:
        st.error("API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not video_id:
        st.error("Video IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # ë©”ì¸ ì‘ì—… ì‹œì‘
        run_step12(video_url)
        with st.spinner(f"ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ID: {video_id})"):
            df = get_video_comments(api_key_input, video_id, max_comments)
        
        if not df.empty:
            st.divider()
            st.subheader("[STEP 3] ëŒ“ê¸€ ê°ì • ë¶„ì„")
            st.success(f"ì´ {len(df)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            
            with st.spinner("í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘..."):
                result_df, nouns = analyze_comments(df)
            
            # 1. ê°ì„± ë¶„ì„ ê²°ê³¼
            st.divider()
            st.subheader("ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼")
            col1, col2 = st.columns([1, 1])
            
            with col1:
                sentiment_counts = result_df['Sentiment'].value_counts()
                fig1, ax1 = plt.subplots()
                colors = ['#ff9999', '#66b3ff', '#99ff99']
                ax1.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)
                ax1.axis('equal')  # ì›í˜• ìœ ì§€
                st.pyplot(fig1)
            
            with col2:
                st.write("#### ê°ì„± ìš”ì•½")
                st.dataframe(sentiment_counts, use_container_width=True)
                st.metric("ê¸ì • ëŒ“ê¸€ ìˆ˜", len(result_df[result_df['Sentiment'] == 'Positive']))
                st.metric("ë¶€ì • ëŒ“ê¸€ ìˆ˜", len(result_df[result_df['Sentiment'] == 'Negative']))

            # 2. ì›Œë“œ í´ë¼ìš°ë“œ
            st.divider()
            st.subheader("â˜ï¸ ì£¼ìš” í‚¤ì›Œë“œ (Word Cloud)")
            if nouns:
                count = Counter(nouns)
                tags = count.most_common(50)
                
                # í°íŠ¸ ê²½ë¡œ í™•ì¸ ë° ì˜ˆì™¸ ì²˜ë¦¬
                if not os.path.exists(font_path):
                    st.warning(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
                    # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© ì‹œë„ (í•œê¸€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)
                    font_path = None 

                wc = WordCloud(font_path=font_path,
                               background_color='white', 
                               width=800, height=600)
                cloud = wc.generate_from_frequencies(dict(tags))
                
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(cloud)
                ax2.axis('off')
                st.pyplot(fig2)
            else:
                st.warning("ë¶„ì„í•  ëª…ì‚¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # 3. ì›ë³¸ ë°ì´í„° (Expander ì‚¬ìš©)
            st.divider()
            with st.expander("ğŸ“ ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë°ì´í„° ë³´ê¸°"):
                st.dataframe(result_df)
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = result_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f'youtube_comments_{video_id}.csv',
                    mime='text/csv',
                )

            # # 4. ì‹œê°„ë³„ ì°¨íŠ¸
            # result_df['Date'] = pd.to_datetime(result_df['Date'])
            # # ì¼ë³„(D) ë˜ëŠ” ì‹œê°„ë³„(H)ë¡œ ë¦¬ìƒ˜í”Œë§í•˜ì—¬ ê°ì„± ì ìˆ˜ í‰ê·  ë‚´ê¸°
            # # (Positive=1, Negative=-1, Neutral=0 ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ í‰ê·  ê³„ì‚°)
            # sentiment_map = {'Positive': 1, 'Negative': -1, 'Neutral': 0}
            # result_df['Score'] = result_df['Sentiment'].map(sentiment_map)
            # daily_sentiment = result_df.set_index('Date').resample('D')['Score'].mean()
            # st.subheader("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ì—¬ë¡  ë³€í™”")
            # st.line_chart(daily_sentiment)

        else:
            st.warning("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Video IDë‚˜ API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
