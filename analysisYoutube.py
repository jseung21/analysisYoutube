# -*- coding: utf-8 -*-
# 1. Konlpy Import First to avoid JVM/DLL conflicts
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg') # Streamlit í™˜ê²½ì—ì„œ GUI ì—ëŸ¬ ë°©ì§€
import seaborn as sns
from googleapiclient.discovery import build
import re
import os

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•¨)
st.set_page_config(
    page_title="YouTube ëŒ“ê¸€ ë¶„ì„ê¸°",
    page_icon="ğŸ¬",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False

# ==========================================
# 1. í•¨ìˆ˜ ì •ì˜ (ìºì‹± ì ìš©)
# ==========================================

@st.cache_data
def get_video_comments(api_key, video_id, max_results=100):
    """YouTube APIë¥¼ í†µí•´ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    try:
        youtube = build('youtube', 'v3', developerKey=api_key)
        comments = []
        
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=min(max_results, 100), # API í•œ ë²ˆ ìš”ì²­ ìµœëŒ€ 100ê°œ
            textFormat="plainText"
        )

        while request and len(comments) < max_results:
            response = request.execute()
            
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']
                date = item['snippet']['topLevelComment']['snippet']['publishedAt']
                comments.append([date, author, comment])
                
            if 'nextPageToken' in response and len(comments) < max_results:
                request = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=min(max_results - len(comments), 100),
                    textFormat="plainText",
                    pageToken=response['nextPageToken']
                )
            else:
                break
                
        return pd.DataFrame(comments, columns=['Date', 'Author', 'Comment'])
    except Exception as e:
        st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

@st.cache_data
def analyze_comments(df):
    """ìˆ˜ì§‘ëœ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ ê°ì„± ì ìˆ˜ì™€ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    okt = Okt()
    
    positive_keywords = ['ì¢‹ë‹¤', 'ë©‹ì§€ë‹¤', 'ìµœê³ ', 'ì‘ì›', 'ì‚¬ë‘', 'ì¬ë¯¸', 'ê°ë™', 'ê¿€íŒ', 'ì„±ê³µ']
    negative_keywords = ['ì‹«ë‹¤', 'ìµœì•…', 'ë…¸ì¼', 'ë°˜ëŒ€', 'ì‹¤ë§', 'ìš°ë ¤', 'ì“°ë ˆê¸°', 'ë³„ë¡œ', 'í™”ë‚¨']

    valid_comments = []
    valid_dates = []
    valid_authors = []
    sentiments = []
    all_nouns = []

    # ì§„í–‰ë¥  í‘œì‹œì¤„
    progress_bar = st.progress(0)
    total_rows = len(df)

    for i, row in df.iterrows():
        comment = row['Comment']
        clean_text = re.sub(r'[^ê°€-í£\s]', '', comment) 
        
        if not clean_text.strip():
            continue
            
        nouns = okt.nouns(clean_text)
        all_nouns.extend([n for n in nouns if len(n) > 1])
        
        score = 0
        for word in clean_text.split():
            if any(pos in word for pos in positive_keywords):
                score += 1
            elif any(neg in word for neg in negative_keywords):
                score -= 1
        
        if score > 0: sentiment = 'Positive'
        elif score < 0: sentiment = 'Negative'
        else: sentiment = 'Neutral'
        
        valid_comments.append(clean_text)
        valid_dates.append(row['Date'])
        valid_authors.append(row['Author'])
        sentiments.append(sentiment)
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if (i + 1) % 10 == 0 or (i + 1) == total_rows:
            progress_bar.progress((i + 1) / total_rows)

    progress_bar.empty() # ì™„ë£Œ í›„ ì œê±°

    result_df = pd.DataFrame({
        'Date': valid_dates,
        'Author': valid_authors,
        'Comment': valid_comments,
        'Sentiment': sentiments
    })
    
    return result_df, all_nouns

# ==========================================
# 2. UI êµ¬ì„±
# ==========================================

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    api_key_input = st.text_input("YouTube API Key", value='AIzaSyDQsGvOtDZfe6nFDdjcxZkybcpKTJ9Z-BI', type="password")
    max_comments = st.slider("ìˆ˜ì§‘í•  ëŒ“ê¸€ ìˆ˜", min_value=10, max_value=1000, value=200, step=10)
    st.info("API KeyëŠ” ê¸°ë³¸ê°’ì´ ì…ë ¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ í™”ë©´
st.title("ğŸ¬ YouTube ëŒ“ê¸€ ê°ì„± ë¶„ì„ê¸°")
st.markdown("""
ìœ íŠœë¸Œ ì˜ìƒì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ **ê¸ì •/ë¶€ì • ì—¬ë¡ **ì„ ë¶„ì„í•˜ê³ , 
ì£¼ìš” í‚¤ì›Œë“œë¥¼ **ì›Œë“œí´ë¼ìš°ë“œ**ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
""")

video_id_input = st.text_input("YouTube Video ID ë˜ëŠ” URL ì…ë ¥", value="QkGkE9jRX_g")

# URLì—ì„œ ID ì¶”ì¶œ ë¡œì§
if "youtube.com" in video_id_input or "youtu.be" in video_id_input:
    if "v=" in video_id_input:
        video_id = video_id_input.split("v=")[1].split("&")[0]
    elif "youtu.be" in video_id_input:
        video_id = video_id_input.split("/")[-1]
    else:
        video_id = video_id_input
else:
    video_id = video_id_input

if st.button("ë¶„ì„ ì‹œì‘ ğŸš€", type="primary"):
    if not api_key_input:
        st.error("API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not video_id:
        st.error("Video IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ID: {video_id})"):
            df = get_video_comments(api_key_input, video_id, max_comments)
        
        if not df.empty:
            st.success(f"ì´ {len(df)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            
            with st.spinner("í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘..."):
                result_df, nouns = analyze_comments(df)
            
            # 1. ê°ì„± ë¶„ì„ ê²°ê³¼
            st.divider()
            st.subheader("ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼")
            col1, col2 = st.columns([1, 1])
            
            with col1:
                sentiment_counts = result_df['Sentiment'].value_counts()
                fig1, ax1 = plt.subplots()
                colors = ['#ff9999', '#66b3ff', '#99ff99']
                ax1.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)
                ax1.axis('equal')  # ì›í˜• ìœ ì§€
                st.pyplot(fig1)
            
            with col2:
                st.write("#### ê°ì„± ìš”ì•½")
                st.dataframe(sentiment_counts, use_container_width=True)
                st.metric("ê¸ì • ëŒ“ê¸€ ìˆ˜", len(result_df[result_df['Sentiment'] == 'Positive']))
                st.metric("ë¶€ì • ëŒ“ê¸€ ìˆ˜", len(result_df[result_df['Sentiment'] == 'Negative']))

            # 2. ì›Œë“œ í´ë¼ìš°ë“œ
            st.divider()
            st.subheader("â˜ï¸ ì£¼ìš” í‚¤ì›Œë“œ (Word Cloud)")
            if nouns:
                count = Counter(nouns)
                tags = count.most_common(50)
                
                # í°íŠ¸ ê²½ë¡œ í™•ì¸ ë° ì˜ˆì™¸ ì²˜ë¦¬
                font_path = 'C:/Windows/Fonts/malgun.ttf'
                if not os.path.exists(font_path):
                    st.warning(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
                    # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© ì‹œë„ (í•œê¸€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)
                    font_path = None 

                wc = WordCloud(font_path=font_path,
                               background_color='white', 
                               width=800, height=600)
                cloud = wc.generate_from_frequencies(dict(tags))
                
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                ax2.imshow(cloud)
                ax2.axis('off')
                st.pyplot(fig2)
            else:
                st.warning("ë¶„ì„í•  ëª…ì‚¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # 3. ì›ë³¸ ë°ì´í„° (Expander ì‚¬ìš©)
            st.divider()
            with st.expander("ğŸ“ ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë°ì´í„° ë³´ê¸°"):
                st.dataframe(result_df)
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = result_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f'youtube_comments_{video_id}.csv',
                    mime='text/csv',
                )
        else:
            st.warning("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Video IDë‚˜ API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
